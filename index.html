<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kumara Kahatapitiya</title>

  <meta name="author" content="Kumara Kahatapitiya">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kumara Kahatapitiya</name>
              </p>
              <p style="text-align:justify">
                I am a Research Scientist at <a href="https://ai.meta.com/meta-ai/" target='_blank'>Meta GenAI</a>. I completed my PhD at <a href="https://www.cs.stonybrook.edu" target='_blank'>Stony Brook University</a>, working on efficient representations for video understanding and generation, advised by
                <a href="http://michaelryoo.com" target='_blank'>Prof. Michael S. Ryoo</a>.

              </p>
              <p style="text-align:justify">
                During my PhD, I interned at <a href="https://research.google" target='_blank'>Google DeepMind</a>, <a href="https://www.qualcomm.com/research/artificial-intelligence/ai-research" target='_blank'>Qualcomm AI Research</a>, and <a href="https://ai.meta.com/meta-ai/" target='_blank'>Meta GenAI</a>. Prior to this, I was a Research Assistant at University of Moratuwa, Sri-Lanka, advised by <a href="http://ranga.staff.uom.lk" target='_blank'>Dr. Ranga Rodrigo</a>, where I also received my Bachelors in Electronic & Telecommunication Engineering.
              </p>
              <p style="text-align:center">
                <a href="https://scholar.google.com/citations?user=ExGkzjQAAAAJ&hl=en">[Google Scholar]</a> &nbsp&nbsp
                <!--<a href="https://www3.cs.stonybrook.edu/~kkahatapitiy/cv.pdf">[CV]</a> &nbsp&nbsp-->
                <a href="https://github.com/kkahatapitiya">[GitHub]</a> &nbsp&nbsp
                <a href="https://twitter.com/kkahatapitiy">[Twitter]</a>
                <br>kkahatapitiy [at] cs.stonybrook.edu
              </p>
              <!--<p style="text-align:center;color:rgb(255,69,0)">
                I am currently on the job market for Research Scientist roles.
              </p>-->
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/kumara.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/kumara.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:10px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[May 2025]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>MarDini</u> was accepted at TMLR and <u>LangRepo</u> was accepted at ACL 2025.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Jan 2025]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>LLaRA</u> and <u>MVU</u> were accepted at ICLR 2025.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Nov 2024]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>AdaCache</u> for accelerating Video Generation was released on arXiv.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Oct 2024]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              Early versions of <u>LangRepo</u> and <u>LVNet</u> were accepted at NeurIPS 2024 workshop on Video-Language Models.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Jul 2024]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>Object-Centric Diffusion</u> for Efficient Video Editing was accepted at ECCV 2024.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Feb 2024]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>Video-conditioned Text Representations</u> for activity recognition was accepted at CVPR 2024.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Oct 2023]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>Grafting Vision Transformers</u> for multi-scale and global information sharing was accepted at WACV 2024.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Apr 2023]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>SWAT</u>, a structure-aware family of token-based models was accepted at IJCAI 2023.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Feb 2023]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>Token Turing Machines</u> for long-term memory in Transformers was accepted at CVPR 2023.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Dec 2022]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>SSDet</u> for weakly-guided Self-supervised detection pretraining was accepted at AAAI 2023.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Jul 2022]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>StARformer</u> with an MDP-like inductive bias for RL was accepted at ECCV 2022 and T-PAMI.
            </td></tr>
          <tr>
            <td style="padding:3px;padding-left:30px;width:15%;vertical-align:middle">[Mar 2022]</td>
            <td style="padding:0;width:85%;vertical-align:middle">
              <u>MS-TCT</u> for temporal action detection with CNN+Transformer embeddings was accepted at CVPR 2022.
            </td></tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;padding-top:40px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/adacache.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Adaptive Caching for Faster Video Generation with Diffusion Transformers</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='https://haozheliu-st.github.io'>Haozhe Liu</a>,
              <a href='https://senhe.github.io'>Sen He</a>,
              <a href='https://scholar.google.com/citations?user=PGtHUI0AAAAJ&hl=en'>Ding Liu</a>,
              <a href='https://kmnp.github.io'>Menglin Jia</a>,
              <a href='https://scholar.google.com/citations?user=XYxv5HIAAAAJ&hl=en'>Chenyang Zhang</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>,
              <a href='https://tianxieusc.github.io'>Tian Xie</a>
              <br>
              <em>arXiv</em> 2024
              <br>
              <a href='https://adacache-dit.github.io'>[project page]</a>
              <a href='https://arxiv.org/abs/2411.02397'>[preprint]</a>
              <a href='https://github.com/AdaCache-DiT/AdaCache'>[code]</a>
              <p></p>
            </td>
          </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;padding-top:40px;width:100%;vertical-align:middle">
              <heading>Selected publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/mardini.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MarDini: Masked Auto-Regressive Diffusion for Video Generation at Scale</papertitle>
              <br>
              <a href='https://haozheliu-st.github.io'>Haozhe Liu</a>,
              <a href='https://shikun.io'>Shikun Liu</a>,
              <a href='https://sites.google.com/view/zijian-zhou/home'>Zijian Zhou</a>,
              <a href='https://xumengmeng.com'>Mengmeng Xu</a>,
              <a href='https://www.linkedin.com/in/yanping-xie-53583428/'>Yanping Xie</a>,
              <a href='https://brandonhan.uk'>Xiao Han</a>,
              <a href='https://www.juancprzs.com'>Juan C. P√©rez</a>,
              <a href='https://scholar.google.com/citations?user=PGtHUI0AAAAJ&hl=en'>Ding Liu</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://kmnp.github.io'>Menglin Jia</a>,
              <a href='https://www.linkedin.com/in/jerry-wu-02143a22/'>Jui-Chieh Wu</a>,
              <a href='https://senhe.github.io'>Sen He</a>,
              <a href='https://scholar.google.co.uk/citations?user=MeS5d4gAAAAJ&hl=en'>Tao Xiang</a>,
              <a href='https://people.idsia.ch/~juergen/'>J√ºrgen Schmidhuber</a>,
              <a href='https://www.linkedin.com/in/juan-manuel-perez-rua-6a639837/'>Juan-Manuel P√©rez-R√∫a</a>
              <br>
              <em>TMLR</em>
              <br>
              <a href='https://mardini-vidgen.github.io'>[project page]</a>
              <a href='https://arxiv.org/abs/2410.20280'>[paper]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/langrepo.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Language Repository for Long Video Understanding</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='https://kahnchana.github.io'>Kanchana Ranasinghe</a>,
              <a href='https://www.linkedin.com/in/jongwpark/'>Jongwoo Park</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>ACL</em> 2025 Findings
              <br>
              <a href='https://arxiv.org/abs/2403.14622'>[paper]</a>
              <a href='https://github.com/kkahatapitiya/LangRepo'>[code]</a>
              <a href='https://www.youtube.com/watch?v=8qn34h-npn8&t=1443s'>[webinar]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/llara.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LLaRA: Supercharging Robot Learning Data for Vision-Language Policy</papertitle>
              <br>
              <a href='https://xxli.me/'>Xiang Li</a>,
              <a href='https://openreview.net/profile?id=~Cristina_Mata1'>Cristina Mata</a>,
              <a href='https://www.linkedin.com/in/jongwpark/'>Jongwoo Park</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://yjang43.github.io/'>Yoo Sung Jang</a>,
              <a href='https://elicassion.github.io/'>Jinghuan Shang</a>,
              <a href='https://kahnchana.github.io'>Kanchana Ranasinghe</a>,
              <a href='https://ryanndagreat.github.io/'>Ryan Burgert</a>,
              <a href='https://pages.cs.wisc.edu/~mucai/'>Mu Cai</a>,
              <a href='https://pages.cs.wisc.edu/~yongjaelee/'>Yong Jae Lee</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>ICLR</em> 2025
              <br>
              <a href='https://arxiv.org/abs/2406.20095'>[paper]</a>
              <a href='https://github.com/LostXine/LLaRA'>[code]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/mvu.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Understanding Long Videos with Multimodal Language Models</papertitle>
              <br>
              <a href='https://kahnchana.github.io'>Kanchana Ranasinghe</a>,
              <a href='https://xxli.me'>Xiang Li</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>ICLR</em> 2025
              <br>
              <a href='https://kahnchana.github.io/mvu'>[project page]</a>
              <a href='https://arxiv.org/abs/2403.16998'>[paper]</a>
              <a href='https://github.com/kahnchana/mvu'>[code]</a>
              <a href='https://www.youtube.com/watch?v=8qn34h-npn8&t=2272s'>[webinar]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/lvnet.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Too many frames, not all useful: Efficient Strategies for Long-form Video QA</papertitle>
              <br>
              <a href='https://www.linkedin.com/in/jongwpark/'>Jongwoo Park</a>,
              <a href='https://kahnchana.github.io'>Kanchana Ranasinghe</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://scholar.google.com/citations?user=gIO_rEkAAAAJ&hl=ko'>Wonjeong Ryoo</a>,
              <a href='https://cs-people.bu.edu/donhk/'>Donghyun Kim</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>NeurIPS</em> 2024 Workshops
              <br>
              <a href='https://arxiv.org/abs/2406.09396'>[paper]</a>
              <a href='https://github.com/jongwoopark7978/LVNet'>[code]</a>
              <a href='https://www.youtube.com/watch?v=8qn34h-npn8&t=185s'>[webinar]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/ocd.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Object-Centric Diffusion for Efficient Video Editing</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='https://www.linkedin.com/in/adil-karjauv/?originalSubdomain=nl'>Adil Karjauv</a>,
              <a href='https://davideabati.info'>Davide Abati</a>,
              <a href='https://yukimasano.github.io'>Yuki M. Asano</a>,
              <a href='https://www.porikli.com'>Fatih Porikli</a>,
              <a href='https://habibian.github.io'>Amirhossein Habibian</a>
              <br>
              <em>ECCV</em> 2024
              <br>
              <a href='https://qualcomm-ai-research.github.io/object-centric-diffusion'>[project page]</a>
              <a href='https://arxiv.org/abs/2401.05735'>[paper]</a>
              <a href='https://drive.google.com/file/d/1DnKIpyhYTS1RfF8ONPCXvo_HvtKTU4lm/view?usp=sharing'>[poster]</a>
              <a href='https://youtu.be/id125PD4Ass?si=MG1Fkf297VYKYFT3'>[talk]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/victr.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VicTR: Video-conditioned Text Representations for Activity Recognition</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='https://anuragarnab.github.io'>Anurag Arnab</a>,
              <a href='https://a-nagrani.github.io'>Arsha Nagrani</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>CVPR</em> 2024
              <br>
              <a href='https://arxiv.org/abs/2304.02560'>[paper]</a>
              <a href='https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202024/29654.png?t=1717488485.9996336'>[poster]</a>
              <a href='https://youtu.be/WJKJuy3wc4s'>[talk]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/graft.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Grafting Vision Transformers</papertitle>
              <br>
              <a href='https://www.linkedin.com/in/jongwpark/'>Jongwoo Park</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://cs-people.bu.edu/donhk/'>Donghyun Kim</a>,
              <a href='https://www.linkedin.com/in/shivchanders/'>Shivchander Sudalairaj</a>,
              <a href='https://www.linkedin.com/in/quanfu-fan-84a2185/'>Quanfu Fan</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>WACV</em> 2024
              <br>
              <a href='https://arxiv.org/abs/2210.15943'>[paper]</a>
              <a href='https://drive.google.com/file/d/159Bvqgwnm5TiWm7c0vHd25OzUXwZvqns/view?usp=share_link'>[poster]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/swat.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>SWAT: Spatial Structure Within and Among Tokens</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>IJCAI</em> 2023
              <br>
              <a href='https://arxiv.org/abs/2111.13677'>[paper]</a>
              <a href='https://github.com/kkahatapitiya/SWAT'>[code]</a>
              <a href='https://drive.google.com/file/d/1DpHLE0WHK_dg9Z031gtktUlJ8_1MAlMO/view?usp=share_link'>[slides]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/ttm.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Token Turing Machines</papertitle>
              <br>
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>,
              <a href='https://keerthanapg.com'>Keerthana Gopalakrishnan</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://tedxiao.me'>Ted Xiao</a>,
              <a href='https://research.google/people/KanishkaRao/'>Kanishka Rao</a>,
              <a href='https://www.linkedin.com/in/austin-charles-stone-1ba33b138/'>Austin Stone</a>,
              <a href='https://research.google/people/107535/'>Yao Lu</a>,
              <a href='https://research.google/people/JulianIbarz/'>Julian Ibarz</a>,
              <a href='https://anuragarnab.github.io'>Anurag Arnab</a>
              <br>
              <em>CVPR</em> 2023
              <br>
              <a href='https://arxiv.org/abs/2211.09119'>[paper]</a>
              <a href='https://github.com/google-research/scenic/tree/main/scenic/projects/token_turing'>[code]</a>
              <a href='https://twitter.com/ryoo_michael/status/1664648204032393219'>[teaser]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/ssdet.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Weakly-guided Self-supervised Pretraining for Temporal Activity Detection</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='http://web.cs.ucla.edu/~zhou.ren/'>Zhou Ren</a>,
              <a href='http://haoxiang.org/academic-homepage/'>Haoxiang Li</a>,
              <a href='https://wuzhenyusjtu.github.io'>Zhenyu Wu</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>,
              <a href='https://www.ganghua.org'>Gang Hua</a>
              <br>
              <em>AAAI</em> 2023
              <br>
              <a href='https://arxiv.org/abs/2111.13675'>[paper]</a>
              <a href='https://github.com/kkahatapitiya/SSDet'>[code]</a>
              <a href='https://www.youtube.com/watch?v=SA1DNjPjGn8'>[talk]</a>
              <a href='https://drive.google.com/file/d/1VwpHiOL3zMfyJ3D-QDUecpZp7NyzkqoZ/view?usp=sharing'>[poster]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/starformer.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>StARformer: Transformer with State-Action-Reward Representations for Visual Reinforcement Learning</papertitle>
              <br>
              <a href='https://www3.cs.stonybrook.edu/~jishang/'>Jinghuan Shang</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://xxli.me'>Xiang Li</a>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>ECCV</em> 2022, <em>TPAMI</em>
              <br>
              <a href='https://arxiv.org/pdf/2110.06206'>[paper]</a>
              <a href='https://ieeexplore.ieee.org/iel7/34/4359286/09878209.pdf'>[journal]</a>
              <a href='https://github.com/elicassion/StARformer'>[code]</a>
              <a href='https://www3.cs.stonybrook.edu/~jishang/starformer/4447.mp4'>[talk]</a>
              <a href='https://www3.cs.stonybrook.edu/~jishang/starformer/4447.pdf'>[poster]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/mstct.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection</papertitle>
              <br>
              <a href='https://dairui01.github.io'>Rui Dai</a>,
              <a href='https://sites.google.com/view/srijan-das/home'>Srijan Das</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>,
              <a href='http://www-sop.inria.fr/members/Francois.Bremond/'>Francois Bremond</a>
              <br>
              <em>CVPR</em> 2022
              <br>
              <a href='https://arxiv.org/abs/2112.03902'>[paper]</a>
              <a href='https://github.com/dairui01/MS-TCT'>[code]</a>
              <a href='https://dairui01.github.io/research/cvpr22_MSTCT_poster.pdf'>[poster]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/swift.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Swift: Adaptive Video Streaming with Layered Neural Codecs</papertitle>
              <br>
              <a href='https://www3.cs.stonybrook.edu/~mdasari/'>Mallesham Dasari</a>,
              <u>Kumara Kahatapitiya</u>,
              <a href='https://www3.cs.stonybrook.edu/~samir/'>Samir Das</a>,
              <a href='https://www3.cs.stonybrook.edu/~arunab/'>Aruna Balasubramanian</a>,
              <a href='https://www3.cs.stonybrook.edu/~samaras/'>Dimitris Samaras</a>
              <br>
              <em>NSDI</em> 2022
              <br>
              <a href='https://www3.cs.stonybrook.edu/~mdasari/papers/nsdi-2022-paper.pdf'>[paper]</a>
              <a href='https://github.com/VideoForage/swift'>[code]</a>
              <a href='https://www3.cs.stonybrook.edu/~mdasari/papers/nsdi-2022-slides.pptx'>[slides]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/coarse_fine.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Coarse-Fine Networks for Temporal Activity Detection in Videos</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='http://michaelryoo.com'>Michael S. Ryoo</a>
              <br>
              <em>CVPR</em> 2021
              <br>
              <a href='https://openaccess.thecvf.com/content/CVPR2021/papers/Kahatapitiya_Coarse-Fine_Networks_for_Temporal_Activity_Detection_in_Videos_CVPR_2021_paper.pdf'>[paper]</a>
              <a href='https://github.com/kkahatapitiya/Coarse-Fine-Networks'>[code]</a>
              <a href='https://www.youtube.com/watch?v=9H_df-_yo78&t=2s'>[talk]</a>
              <a href='https://drive.google.com/file/d/1GstdpLycWE4yLFrlxk5FUTtLZ8KVD7Ox/view?usp=sharing'>[poster]</a>
              <p></p>
            </td>
          </tr>

          <tr bgcolor="#ffffff">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div><img src='images/linearconv.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Exploiting the Redundancy in Convolutional Filters for Parameter Reduction</papertitle>
              <br>
              <u>Kumara Kahatapitiya</u>,
              <a href='http://ranga.staff.uom.lk'>Ranga Rodrigo</a>
              <br>
              <em>WACV</em> 2021
              <br>
              <a href='https://openaccess.thecvf.com/content/WACV2021/papers/Kahatapitiya_Exploiting_the_Redundancy_in_Convolutional_Filters_for_Parameter_Reduction_WACV_2021_paper.pdf'>[paper]</a>
              <a href='https://github.com/kkahatapitiya/LinearConv'>[code]</a>
              <a href='https://www.youtube.com/watch?v=yPGs3B2mdGA'>[talk]</a>
              <p></p>
            </td>
          </tr>

        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Other Projects</heading>
              <p style="text-align:left">
              <ul style="list-style: none; padding-left: 10px; text-align:justify">
                <li><b>X3D-Multigrid</b> <a href='https://github.com/kkahatapitiya/X3D-Multigrid'>[code]</a> <br>
                  A PyTorch implementation for "X3D: Expanding Architectures for Efficient Video Recognition models" [CVPR2020] with
                  "A Multigrid Method for Efficiently Training Video Models" [CVPR2020]. In contrast to the original repository by FAIR,
                  this repository provides a simpler, less modular and more familiar structure of implementation for faster and easier adoptation.</li>
                <li><b>Optimal Transport in NumPy </b> <a href='https://github.com/kkahatapitiya/numpy_ot'>[code]</a> <br>
                  This repository contrains a few Optimal Transport Algorithms implemented using NumPy, including
                  "A Direct O(1/epsilon) Iteration Parallel Algorithm for Optimal Transport" [NeurIPS2019],
                  "Computational Optimal Transport: Complexity by Accelerated Gradient Descent is better than by Sinkhorn's Algorithm" [PMLR2018] and
                  "Lightspeed Computation of Optimal Transport" [NeurIPS2013].</li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;padding-top:0;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p style="text-align:left">
              <ul style="list-style: none; padding-left: 10px;">
                <li>CSE327: Computer Vision - TA <a href='https://www3.cs.stonybrook.edu/~cse327/'>(Spring 2020)</a> </li>
                <li>CSE215: Foundations of Computer Science - TA <a href='https://www3.cs.stonybrook.edu/~omkant/215f19.html'>(Fall 2019)</a> </li>
              </ul>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> for the template.
                </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
